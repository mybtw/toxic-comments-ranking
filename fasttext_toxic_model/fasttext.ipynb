{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.metrics import AUC\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "print(\"GPU devices: \", gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "#tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ru = fasttext.load_model('C:\\\\Users\\\\astaf\\\\toxicComments\\\\fastTextModel\\\\cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../dataset/dataset_lg_train_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['processed_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed_comment'][248295]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 254288\n",
    "maxlen = 300\n",
    "# Инициализация токенизатора\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['processed_comment'])\n",
    "\n",
    "# Преобразование текстов в последовательности\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_comment'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=300, truncating='post', padding='post')\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, data['toxicity'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Найти индексы строк с меткой toxicity = 1\n",
    "toxic_indices_train = np.where(y_train == 1)[0]\n",
    "\n",
    "# Дублировать эти строки в тренировочной выборке\n",
    "X_train_toxic = X_train[toxic_indices_train]\n",
    "y_train_toxic = y_train.iloc[toxic_indices_train]\n",
    "\n",
    "# Объединить исходные тренировочные данные с новыми дублированными строками\n",
    "X_train_balanced = np.concatenate([X_train, X_train_toxic], axis=0)\n",
    "y_train_balanced = np.concatenate([y_train, y_train_toxic], axis=0)\n",
    "\n",
    "# Удвоить строки с меткой toxicity = 1\n",
    "#X_train_balanced = np.concatenate([X_train_balanced, X_train_toxic], axis=0)\n",
    "#y_train_balanced = np.concatenate([y_train_balanced, y_train_toxic], axis=0)\n",
    "\n",
    "# Перемешать данные, чтобы сохранить случайность\n",
    "shuffle_indices_train = np.random.permutation(len(X_train_balanced))\n",
    "X_train_balanced_shuffled = X_train_balanced[shuffle_indices_train]\n",
    "y_train_balanced_shuffled = y_train_balanced[shuffle_indices_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer_bilstm_untrainableembedding.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "# Инициализация матрицы эмбеддингов\n",
    "embedding_matrix = np.zeros((max_features, embed_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = model_ru[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        print(f'Error creating embedding for {word}')\n",
    "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_matrix_fasttext_untrainableembedding.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "# Ранний выход\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='max', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('embedding_matrix_fasttext.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), trainable = False)(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', AUC(name='roc_auc', curve='ROC'),tfa.metrics.F1Score(num_classes=1, threshold=0.5), precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape:\", X_train_balanced_shuffled.shape)\n",
    "print(\"Test data shape:\", y_train_balanced_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('bilstm.weights_untrainableembedding.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('fasttext.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fasttext_final.keras', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.flatten() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "roc_auc = roc_auc_score(y_test, predictions)\n",
    "accuracy = accuracy_score(y_test, (predictions > 0.5).astype(int))\n",
    "report = classification_report(y_test, (predictions > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_lstm.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"ты ужасный человек, стоит плох что-то еще\", \"Я желаю чтобы ты утонул в колодце тюлень\"]\n",
    "\n",
    "# Преобразование предложений в последовательности\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=maxlen, truncating='post', padding='post')\n",
    "\n",
    "# Предсказание модели\n",
    "predictions = model.predict(test_padded)\n",
    "predictions = predictions.flatten()  # Преобразование в одномерный массив, если модель возвращает двумерный\n",
    "\n",
    "# Вывод результатов\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"Sentence: '{sentence}' - Prediction (Toxic Probability): {predictions[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
