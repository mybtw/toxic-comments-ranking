{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4dd8bd7a80a0d9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from html.parser import HTMLParser\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import string\n",
    "import fasttext\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy.lang.ru import Russian\n",
    "import concurrent.futures\n",
    "\n",
    "contractions = {\n",
    "    \"–∏ —Ç.–¥.\": \" –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ \",\n",
    "    \"–∏ —Ç.–ø.\": \" –∏ —Ç–æ–º—É –ø–æ–¥–æ–±–Ω–æ–µ \",\n",
    "    \"—É–ª.\": \" —É–ª–∏—Ü–∞ \"\n",
    "}\n",
    "\n",
    "nlp = spacy.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b512259250e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.extracted_data = []\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        self.extracted_data.append(data)\n",
    "\n",
    "    def get_data(self):\n",
    "        return ''.join(self.extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_cyrillic(text):\n",
    "    # –£–¥–∞–ª—è–µ–º –≤—Å–µ, –∫—Ä–æ–º–µ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤\n",
    "    return re.sub(r'[^–∞-—è–ê-–Ø—ë–Å]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_r_with_rubles(text):\n",
    "    # –ó–∞–º–µ–Ω–∞ \"—Ä\" –ø–æ—Å–ª–µ —á–∏—Å–ª–∞\n",
    "    text = re.sub(r'(\\d)—Ä\\b', r'\\1 —Ä—É–±–ª–µ–π ', text)\n",
    "    # –ó–∞–º–µ–Ω–∞ \"—Ä\" –ø–µ—Ä–µ–¥ —á–∏—Å–ª–æ–º\n",
    "    text = re.sub(r'\\b—Ä(\\d)', r' —Ä—É–±–ª–µ–π \\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_currency_symbols(text):\n",
    "    # –°–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤ –≤–∞–ª—é—Ç –∏ –∏—Ö –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π\n",
    "    currency_symbols = {\n",
    "        r'\\$': ' –¥–æ–ª–ª–∞—Ä–æ–≤ ',\n",
    "        r'‚Ç¨': ' –µ–≤—Ä–æ ',\n",
    "        r'¬£': ' —Ñ—É–Ω—Ç–æ–≤ ',\n",
    "        r'¬•': ' –π–µ–Ω ',\n",
    "        r'‚ÇΩ': ' —Ä—É–±–ª–µ–π '\n",
    "    }\n",
    "    \n",
    "    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏ –∑–∞–º–µ–Ω—è–µ–º –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –≤–∞–ª—é—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "    for symbol, name in currency_symbols.items():\n",
    "        text = re.sub(symbol, name, text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_list_comp(text):\n",
    "    return re.sub(r'\\s+', ' ', re.sub(r'[\\s{}]+'.format(re.escape(string.punctuation)), ' ', text)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contractions_dict):\n",
    "    for key, value in contractions_dict.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_units_with_full_names(text):\n",
    "    # –°–ª–æ–≤–∞—Ä—å –∑–∞–º–µ–Ω: —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏ –µ–≥–æ –ø–æ–ª–Ω–æ–µ —Å–ª–æ–≤–µ—Å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
    "    units = {\n",
    "        '–∫–≥': '–∫–∏–ª–æ–≥—Ä–∞–º–º',\n",
    "        '–≥': '–≥—Ä–∞–º–º',\n",
    "        '–º': '–º–µ—Ç—Ä',\n",
    "        '—Å–º': '—Å–∞–Ω—Ç–∏–º–µ—Ç—Ä',\n",
    "        '–º–º': '–º–∏–ª–ª–∏–º–µ—Ç—Ä',\n",
    "        '–ª': '–ª–∏—Ç—Ä',\n",
    "        '–º–ª': '–º–∏–ª–ª–∏–ª–∏—Ç—Ä',\n",
    "        '—á': '—á–∞—Å',\n",
    "        '–º–∏–Ω': '–º–∏–Ω—É—Ç–∞',\n",
    "        '—Å–µ–∫': '—Å–µ–∫—É–Ω–¥–∞',\n",
    "        '–∫–º': '–∫–∏–ª–æ–º–µ—Ç—Ä',\n",
    "        '—à—Ç': '—à—Ç—É–∫'\n",
    "    }\n",
    "    \n",
    "    for unit, full_name in units.items():\n",
    "        text = text.replace(f' {unit} ', f' {full_name} ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hyphens(text):\n",
    "    # –ó–∞–º–µ–Ω–∞ –¥–µ—Ñ–∏—Å–∞ –Ω–∞ –º–∏–Ω—É—Å –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏—è—Ö\n",
    "    text = re.sub(r'(?<=\\d)\\-(?=\\d)', ' –º–∏–Ω—É—Å ', text)  # –º–µ–∂–¥—É —Ü–∏—Ñ—Ä–∞–º–∏\n",
    "    text = re.sub(r'(?<=\\s)\\-(?=\\d)', ' –º–∏–Ω—É—Å ', text)  # –ø–µ—Ä–µ–¥ —á–∏—Å–ª–æ–º –ø–æ—Å–ª–µ –ø—Ä–æ–±–µ–ª–∞\n",
    "    \n",
    "    # –ó–∞–º–µ–Ω–∞ –¥–µ—Ñ–∏—Å–∞ –Ω–∞ —Ç–∏—Ä–µ, —É—á–∏—Ç—ã–≤–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –≥–¥–µ –¥–µ—Ñ–∏—Å –Ω–µ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–∞—Å—Ç–∏—Ü—ã)\n",
    "    # –†–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É —Ç–∏—Ä–µ –∏ –¥–µ—Ñ–∏—Å–æ–º –≤ —á–∞—Å—Ç–∏—Ü–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"—á—Ç–æ-—Ç–æ\", \"–∫—Ç–æ-–ª–∏–±–æ\")\n",
    "    text = re.sub(r'\\b(\\w+)-(—Ç–æ|–ª–∏–±–æ|–Ω–∏–±—É–¥—å|—Ç–∞–∫–∏|–∫–∞)\\b', r'\\1-\\2', text)\n",
    "\n",
    "    # –û–±—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ –∑–∞–º–µ–Ω—ã –¥–µ—Ñ–∏—Å–∞ –Ω–∞ —Ç–∏—Ä–µ, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —á–∞—Å—Ç–∏—Ü—ã\n",
    "    text = re.sub(r'(?<!\\w)(\\w+)-(\\w+)', r'\\1 –¥–µ—Ñ–∏—Å \\2', text)\n",
    "\n",
    "    # –ó–∞–º–µ–Ω–∞ –¥–µ—Ñ–∏—Å–∞ –Ω–∞ —Ç–∏—Ä–µ –≤ –ø—Ä–æ—á–∏—Ö —Å–ª—É—á–∞—è—Ö\n",
    "    text = re.sub(r'(?<=\\s)-(?=\\s)', ' –¥–µ—Ñ–∏—Å ', text)  # –º–µ–∂–¥—É –ø—Ä–æ–±–µ–ª–∞–º–∏\n",
    "    text = re.sub(r'(?<=\\w)-(?=\\s)', ' –¥–µ—Ñ–∏—Å ', text)  # –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–¥ –ø—Ä–æ–±–µ–ª–æ–º\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces_regex(text):\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_identifiers(text):\n",
    "    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∏—â–µ—Ç —à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç–µ—Ä–∏—á–Ω—ã–µ —Ö–µ—à–∏, UUIDs –∏ –¥—Ä—É–≥–∏–µ —Ç–∏–ø–∏—á–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã\n",
    "    pattern = r'\\b([a-f0-9]{32}|[a-f0-9]{40}|[a-f0-9]{64}|[a-f0-9-]{36}|[a-zA-Z0-9-]{7,})\\b'\n",
    "    \n",
    "    # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ —Å–ª–æ–≤–æ \"–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä\"\n",
    "    return re.sub(pattern, ' –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä ', text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_phone_numbers_and_digits(text):\n",
    "    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –Ω–æ–º–µ—Ä–æ–≤ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤\n",
    "    phone_pattern = r'\\+?\\d[\\d\\-\\(\\)\\.\\s]{8,}\\d'\n",
    "    \n",
    "    # –ó–∞–º–µ–Ω–∏—Ç—å –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –Ω–∞ \"–Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞\"\n",
    "    text = re.sub(phone_pattern, ' –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ ', text)\n",
    "\n",
    "    text = replace_identifiers(text)\n",
    "    \n",
    "    # –ó–∞–º–µ–Ω–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —á–∏—Å–ª–∞ –Ω–∞ \"—á–∏—Å–ª–æ\"\n",
    "    text = re.sub(r'\\b\\d+\\b', ' —á–∏—Å–ª–æ ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_custom_text_emojis(text):\n",
    "    # –°–ª–æ–≤–∞—Ä—å —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–æ–¥–∑–∏ –∏ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π\n",
    "    emoji_dict = {\n",
    "        r':\\)': ' —É–ª—ã–±–∫–∞ ',\n",
    "        r':\\(': ' –≥—Ä—É—Å—Ç–Ω–æ–µ –ª–∏—Ü–æ ',\n",
    "        r':D': ' —Å–º–µ—Ö ',\n",
    "        r';\\)': ' –ø–æ–¥–º–∏–≥–∏–≤–∞–Ω–∏–µ '\n",
    "    }\n",
    "    \n",
    "    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏ –∑–∞–º–µ–Ω—è–µ–º –∫–∞–∂–¥—ã–π —ç–º–æ–¥–∑–∏ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "    for emoji, description in emoji_dict.items():\n",
    "        text = re.sub(emoji, description, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19334067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_math_symbols_with_words(text):\n",
    "    # –°–ª–æ–≤–∞—Ä—å –∑–∞–º–µ–Ω: –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∏–º–≤–æ–ª –∏ –µ–≥–æ —Å–ª–æ–≤–µ—Å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
    "    math_symbols = {\n",
    "        '+': ' –ø–ª—é—Å ',\n",
    "        '*': ' —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ ',\n",
    "        '/': ' —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ ',\n",
    "        '=': ' —Ä–∞–≤–Ω–æ ',\n",
    "        '<': ' –º–µ–Ω—å—à–µ ',\n",
    "        '>': ' –±–æ–ª—å—à–µ ',\n",
    "        '‚â§': ' –º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ ',\n",
    "        '‚â•': ' –±–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ '\n",
    "    }\n",
    "    \n",
    "    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏ –∑–∞–º–µ–Ω—è–µ–º –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "    for symbol, word in math_symbols.items():\n",
    "        text = text.replace(symbol, word)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d872864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_slang(text):\n",
    "    # –°–ª–æ–≤–∞—Ä—å –∑–∞–º–µ–Ω: —Å–ª–µ–Ω–≥ –∏ –µ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
    "    slang_dict = {\n",
    "        r'\\b—Ç–≥\\b': ' —Ç–µ–ª–µ—Ñ–æ–Ω ',\n",
    "        r'\\b—Å–ø—Å\\b': ' —Å–ø–∞—Å–∏–±–æ ',\n",
    "        r'\\b–ø–ª–∏–∑\\b': ' –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ ',\n",
    "        r'\\b—á–µ–ª\\b': ' —á–µ–ª–æ–≤–µ–∫ ',\n",
    "        r'\\b–∫–µ–∫\\b': ' —Å–º–µ—à–Ω–æ ',\n",
    "        r'\\b–ª–æ–ª\\b': ' —Å–º–µ—à–Ω–æ '\n",
    "    }\n",
    "    \n",
    "    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏ –∑–∞–º–µ–Ω—è–µ–º –∫–∞–∂–¥—ã–π —Å–ª–µ–Ω–≥ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "    for slang, normal in slang_dict.items():\n",
    "        text = re.sub(slang, normal, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "   # text = replace_hyphens(text)\n",
    "\n",
    "    #print(\"hyphens \", text)\n",
    "\n",
    "    text = replace_slang(text)\n",
    "\n",
    "    #print(\"slang \", text)\n",
    "\n",
    "    text = replace_custom_text_emojis(text)\n",
    "\n",
    "    #print(\"emoji \", text)\n",
    "\n",
    "    text = replace_r_with_rubles(text)\n",
    "\n",
    "    #print(\"rubles \", text)\n",
    "\n",
    "    text = replace_currency_symbols(text)\n",
    "\n",
    "    #print(\"currency \", text)\n",
    "\n",
    "    text = replace_phone_numbers_and_digits(text)\n",
    "\n",
    "    #print(\"phone_numbers_and_digits \", text)\n",
    "\n",
    "    # –ü–µ—Ä–µ–≤–æ–¥ –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä\n",
    "\n",
    "    \n",
    "    # –†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\n",
    "    text = expand_contractions(text, contractions)\n",
    "\n",
    "    \"\"\"   url_pattern = re.compile(\n",
    "    r'\\b(?:https?|ftp|mailto|data|tel):\\/\\/'  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å—Ö–µ–º—ã\n",
    "    r'(?:(?:[a-z0-9-]+\\.)+[a-z]{2,13})'  # –î–æ–º–µ–Ω–Ω–æ–µ –∏–º—è\n",
    "    r'(?:\\/[\\w\\-\\.~:+\\/?#\\[\\]@!$&\\'()*;,=]*)?'  # –ü—É—Ç—å\n",
    "    r'(?:(?:\\?[\\w\\-\\.~:+\\/?#\\[\\]@!$&\\'()*;,=]*)?)'  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "    r'(?:(?:#[\\w\\-]*)?)\\b',  # –Ø–∫–æ—Ä—å\n",
    "    re.IGNORECASE)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    url_pattern = re.compile(r'https?://(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}')\n",
    "\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "\n",
    "    #print(\"links \", text)\n",
    "\n",
    "    # –ø–µ—Ä–µ–≤–æ–¥–∏–º —ç–º–æ–¥–∂–∏ –≤ —Ç–µ–∫—Å—Ç –≤–∏–¥–∞ :–º–∞—à–µ—Ç_—Ä—É–∫–æ–π: , –∑–∞–º–µ–Ω—è–µ–º —Å–∏–º–≤–æ–ª—ã : –∏ _ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = emoji.demojize(text, language='ru').replace(':', ' ').replace('_', ' ')\n",
    "\n",
    "    #print(\"emoji  \", text)\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ HTML\n",
    "    parser = MyHTMLParser()\n",
    "    parser.feed(text)\n",
    "    extracted_text = parser.get_data()\n",
    "    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É\n",
    "    if extracted_text != text:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ HTML —Ç–µ–≥\n",
    "        text = preprocess_text(extracted_text)\n",
    "    \n",
    "    #print(\"html  \", text)\n",
    "    \n",
    "    # —É–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
    "    text = remove_punctuation_list_comp(replace_math_symbols_with_words(text))\n",
    "\n",
    "    #print(\"remove_punctuation_list_comp  \", text)\n",
    "\n",
    "    text = replace_units_with_full_names(text)\n",
    "\n",
    "    #print(\"replace_units_with_full_names  \", text)\n",
    "\n",
    "    text = remove_extra_spaces_regex(remove_non_cyrillic(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_texts(texts):\n",
    "    lemmatized_texts = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "        lemmatized_texts.append(lemmatized_text)\n",
    "    return lemmatized_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_and_lemmatize(input_file_path, output_file_path):\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞\n",
    "    if 'processed_comment' not in df.columns:\n",
    "        raise ValueError(\"–í —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'processed_comment'.\")\n",
    "\n",
    "    # –ü–∞–∫–µ—Ç–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n",
    "    df['lemmatized_comment'] = lemmatize_texts(df['processed_comment'])\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π DataFrame –≤ –Ω–æ–≤—ã–π CSV —Ñ–∞–π–ª\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06840f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \" –ü—Ä–∏–≤–µ—Ç :D –ö–∞–∫ –¥–µ–ª–∞? :) –ù–∞–¥–µ—é—Å—å,  ) –≤—Å—ë?—Ö–æ—Ä–æ—à–æ:($100, ‚Ç¨75, ¬£50, f wa awfx —Å–ø—Å ¬• 5000 –∏ ‚ÇΩ3000 –í–∞—à –∞–ø–∏.–∫–ª—é—á: abcd1234 ef5678gh, –≤–∞—à UUID: 123e4567-e89b-12d3-a456-426614174000. +7 123 456-78-90 –∏–ª–∏ (123) 456 7890. –í–æ–∑—Ä–∞—Å—Ç 30 –ª–µ—Ç –≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è–º–∏ —Ç–∏–ø–∞ –∫–≥ –∏ —Ç.–¥. –∏ HTML + <b>—Ç–µ–≥–∞–º–∏</b>. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –Ω–∞ www.foufos.gr  –ü—Ä–∏–≤–µ—Ç üëã! –Ø —Ä–∞–¥ —ç—Ç–æ–º—É –¥–Ω—é üòä\"\n",
    "processed_text = preprocess_text(sample_text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_potential_contractions(file_path, column_name='comments'):\n",
    "    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\n",
    "    contractions_pattern = re.compile(r'\\b\\w+\\.\\b')\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∞ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"–ö–æ–ª–æ–Ω–∫–∞ {column_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ.\")\n",
    "    \n",
    "    # –ü–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤ –∫–∞–∂–¥–æ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏\n",
    "    df['potential_contractions'] = df[column_name].apply(lambda x: re.findall(contractions_pattern, x))\n",
    "    \n",
    "    return df[df['potential_contractions'].map(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset/dataset-ru2.csv'\n",
    "result_df = find_potential_contractions(file_path, 'comment')\n",
    "result_df.to_csv('dataset/look2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b31ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_add_column(input_file_path, output_file_path):\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    if 'comment' not in df.columns:\n",
    "        raise ValueError(\"–í —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'comment'.\")\n",
    "\n",
    "    df['processed_comment'] = df['comment'].apply(preprocess_text)\n",
    "    \n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eaac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'dataset/dataset_lg.csv'  # –£–∫–∞–∂–∏—Ç–µ –∑–¥–µ—Å—å –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É\n",
    "output_file = 'dataset/dataset_lg_train.csv'  # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "\n",
    "process_csv_add_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eced9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'dataset_lg_train.csv'\n",
    "output_file_path = 'dataset/dataset_lg_train_final.csv'\n",
    "process_csv_and_lemmatize(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/train-ru2-lemmatized.csv')\n",
    "print(df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "df = pd.read_csv('train-ru-lemmatized.csv')\n",
    "\n",
    "# –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–∞ (–∑–∞–º–µ–Ω–∏—Ç–µ 'column_name' –Ω–∞ –∏–º—è –≤–∞—à–µ–≥–æ —Å—Ç–æ–ª–±—Ü–∞)\n",
    "column = df['lemmatized_comment']\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "# –ü–æ–¥—Å—á—ë—Ç —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ —Å—Ç–æ–ª–±—Ü–∞\n",
    "df['word_count'] = column.apply(lambda x: unique_words.update(str(x).lower().split()))\n",
    "\n",
    "# –ü–æ–¥—Å—á—ë—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ —Å—Ç–æ–ª–±—Ü–µ\n",
    "print(column.apply(lambda x: len(str(x))).max())\n",
    "\n",
    "print(column.apply(lambda x: len(str(x))).mean())\n",
    "\n",
    "print(\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Å—Ç–æ–ª–±—Ü–µ:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b700704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train-ru-lemmatized.csv')\n",
    "\n",
    "df['word_count'] = df['lemmatized_comment'].str.split().apply(len)\n",
    "\n",
    "# –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —è—á–µ–π–∫–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–≤\n",
    "max_words_index = df['word_count'].idxmax()\n",
    "\n",
    "# –í—ã–≤–æ–¥ —è—á–µ–π–∫–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–≤\n",
    "print(\"–Ø—á–µ–π–∫–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–≤:\")\n",
    "print(df.loc[max_words_index, 'lemmatized_comment'])\n",
    "\n",
    "# –í—ã–≤–æ–¥ –≤—Å–µ–π —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
    "print(\"\\n–°—Ç—Ä–æ–∫–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–≤:\")\n",
    "print(df.loc[max_words_index])\n",
    "print(df['comment'][250236])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train-ru-lemmatized.csv')\n",
    "df['word_count'] = df['lemmatized_comment'].str.split().apply(len)\n",
    "\n",
    "# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤\n",
    "average_word_count = df['word_count'].mean()\n",
    "\n",
    "print(\"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —è—á–µ–π–∫–µ:\", average_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('dataset/dataset-ru1.csv')\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Ç–æ—Ä–æ–≥–æ CSV-—Ñ–∞–π–ª–∞\n",
    "df2 = pd.read_csv('dataset/dataset-ru2.csv')\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ DataFrame\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ DataFrame –≤ –Ω–æ–≤—ã–π CSV-—Ñ–∞–π–ª\n",
    "combined_df.to_csv('dataset/dataset_lg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ee6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ nlp\n",
    "    doc = nlp(text)\n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a45484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatize_text('–≤–æ—Ç –ø—Ä—è–º –±–µ—Å—è—Ç –∏—Ö –ø–µ—Å–Ω–∏ –∞–∂ –±–ª–µ–≤–∞—Ç—å –≤—Å–µ–≥–¥–∞ —Ö–æ—á–µ—Ç—Å—è –∫–ª–æ—É–Ω'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977868eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('dataset/dataset-ru2.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad558f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤\n",
    "df1 = pd.read_csv('dataset/dataset-ru1.csv')\n",
    "df2 = pd.read_csv('dataset/dataset-ru2.csv')\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "combined_df.to_csv('dataset/dataset-ru.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642762f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('dataset/dataset-ru3.csv')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2185cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['comment'][239000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['preprocessed_comment'] = df2['comment'].apply(preprocess_text)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–≥–æ DataFrame –≤ –Ω–æ–≤—ã–π CSV —Ñ–∞–π–ª\n",
    "df2.to_csv('updated_file-april.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df2['preprocessed_comment'].duplicated(keep=False)\n",
    "\n",
    "# –í—ã–≤–æ–¥ —Å—Ç—Ä–æ–∫ —Å –¥—É–±–ª–∏—Ä—É—é—â–∏–º–∏—Å—è –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "duplicate_rows = df2[duplicates]\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fe592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ids = df2[\"preprocessed_comment\"]\n",
    "df2[ids.isin(ids[ids.duplicated()])].sort_values(\"preprocessed_comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('dataset/dataset_lg_train.csv')\n",
    "df.dropna(inplace=True)\n",
    "unique_words = set()\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥ –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ\n",
    "for comment in df2['processed_comment']:\n",
    "    # –î–µ–ª–∞–µ–º —Å–ø–ª–∏—Ç –ø–æ –ø—Ä–æ–±–µ–ª—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ\n",
    "    unique_words.update(comment.split())\n",
    "\n",
    "# –í—ã–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "print(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤:\", len(unique_words))\n",
    "#print(list(unique_words)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('dataset/dataset_lg_train.csv')\n",
    "\n",
    "# –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "df2.dropna(inplace=True)\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "unique_words = set()\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥ –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ\n",
    "for comment in df2['processed_comment']:\n",
    "    # –î–µ–ª–∞–µ–º —Å–ø–ª–∏—Ç –ø–æ –ø—Ä–æ–±–µ–ª—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ\n",
    "    unique_words.update(comment.split())\n",
    "\n",
    "# –í—ã–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "print(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bcaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/dataset-ru2.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "248283 + 14412"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
