{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.layers import  GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "print(\"GPU devices: \", gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_files):\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data_frames.append(df)\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    shuffled_df = combined_df.sample(frac=1, random_state=42)\n",
    "    return shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = ['..\\dataset\\dataset_lg_train_final.csv']\n",
    "data = load_data(csv_files)\n",
    "print(data.isnull().any())\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['processed_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences = data['processed_comment']\n",
    "y = data['toxicity']\n",
    "\n",
    "print(f\"Количество записей в data: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 176404\n",
    "maxlen = 200\n",
    "# Инициализация токенизатора\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['processed_comment'])\n",
    "\n",
    "# Преобразование текстов в последовательности\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_comment'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, truncating='post', padding='post')\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, data['toxicity'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_1_count = np.sum(y_train == 1)\n",
    "print(\"Количество элементов с меткой toxicity = 1 в y_train:\", toxicity_1_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Найти индексы строк с меткой toxicity = 1\n",
    "toxic_indices_train = np.where(y_train == 1)[0]\n",
    "\n",
    "# Дублировать эти строки в тренировочной выборке\n",
    "X_train_toxic = X_train[toxic_indices_train]\n",
    "y_train_toxic = y_train.iloc[toxic_indices_train]\n",
    "\n",
    "# Объединить исходные тренировочные данные с новыми дублированными строками\n",
    "X_train_balanced = np.concatenate([X_train, X_train_toxic], axis=0)\n",
    "y_train_balanced = np.concatenate([y_train, y_train_toxic], axis=0)\n",
    "\n",
    "# Удвоить строки с меткой toxicity = 1\n",
    "#X_train_balanced = np.concatenate([X_train_balanced, X_train_toxic], axis=0)\n",
    "#y_train_balanced = np.concatenate([y_train_balanced, y_train_toxic], axis=0)\n",
    "\n",
    "# Перемешать данные, чтобы сохранить случайность\n",
    "shuffle_indices_train = np.random.permutation(len(X_train_balanced))\n",
    "X_train_balanced_shuffled = X_train_balanced[shuffle_indices_train]\n",
    "y_train_balanced_shuffled = y_train_balanced[shuffle_indices_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced_shuffled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_balanced_shuffled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_1_count = np.sum(y_train_balanced_shuffled == 1)\n",
    "print(\"Количество элементов с меткой toxicity = 1 в y_train:\", toxicity_1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer_lstm_lemmatized.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='max', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, )) \n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', AUC(name='roc_auc', curve='ROC'), tfa.metrics.F1Score(num_classes=1, threshold=0.5), precision, recall])                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(X_train_balanced_shuffled, y_train_balanced_shuffled, batch_size=32, epochs=2, validation_data=(X_test, y_test),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('lstm_upsampled.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text):\n",
    "    tokenized = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(tokenized, maxlen=200)  # убедитесь, что maxlen соответствует тому, что использовалось при обучении\n",
    "    return padded\n",
    "\n",
    "# Пример текста\n",
    "text = \"ну ты и плох\"\n",
    "prepared_text = prepare_input(text)\n",
    "\n",
    "# Предсказание токсичности\n",
    "prediction = model.predict(prepared_text)\n",
    "print(\"Toxicity Score:\", prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.flatten() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "roc_auc = roc_auc_score(y_test, predictions)\n",
    "accuracy = accuracy_score(y_test, (predictions > 0.5).astype(int))\n",
    "report = classification_report(y_test, (predictions > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"ты плохой человек стоит поискать что-то еще\", \"аж блевать клоун\"]\n",
    "\n",
    "# Преобразование предложений в последовательности\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=maxlen, truncating='post', padding='post')\n",
    "\n",
    "# Предсказание модели\n",
    "predictions = model.predict(test_padded)\n",
    "predictions = predictions.flatten()  # Преобразование в одномерный массив, если модель возвращает двумерный\n",
    "\n",
    "# Вывод результатов\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"Sentence: '{sentence}' - Prediction (Toxic Probability): {predictions[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
